{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab43a9f3-7f8f-4ca9-b4cb-916dc48401e0",
   "metadata": {},
   "source": [
    "# 协程"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa99277-4225-4211-9eaa-08ac235e43e1",
   "metadata": {},
   "source": [
    "## 爬虫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33dbed9e-4908-40ae-98d5-4e5fc6d1d8cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "crawling url_1\n",
      "OK url_1\n",
      "crawling url_2\n",
      "OK url_2\n",
      "crawling url_3\n",
      "OK url_3\n",
      "crawling url_4\n",
      "OK url_4\n",
      "CPU times: user 5.15 ms, sys: 3.98 ms, total: 9.13 ms\n",
      "Wall time: 10 s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def crawl_page(url):\n",
    "    print('crawling {}'.format(url))\n",
    "    sleep_time = int(url.split('_')[-1])\n",
    "    time.sleep(sleep_time)\n",
    "    print('OK {}'.format(url))\n",
    "\n",
    "def main(urls):\n",
    "    for url in urls:\n",
    "        crawl_page(url)\n",
    "\n",
    "%time main(['url_1', 'url_2', 'url_3', 'url_4'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee332236-ff8d-4f1e-a04e-f6fa4bb9689c",
   "metadata": {},
   "source": [
    "并发优化："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9946f998-883e-43d4-9445-1086aff39a23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: nest_asyncio in /opt/homebrew/Cellar/jupyterlab/4.3.4_1/libexec/lib/python3.13/site-packages (1.6.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install nest_asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db7f26c9-5308-4c24-9530-cf0fab1caf74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "crawling url_1\n",
      "OK url_1\n",
      "crawling url_2\n",
      "OK url_2\n",
      "crawling url_3\n",
      "OK url_3\n",
      "crawling url_4\n",
      "OK url_4\n",
      "CPU times: user 6.38 ms, sys: 5.01 ms, total: 11.4 ms\n",
      "Wall time: 10 s\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "async def crawl_page(url):\n",
    "    print('crawling {}'.format(url))\n",
    "    sleep_time = int(url.split('_')[-1])\n",
    "    await asyncio.sleep(sleep_time)\n",
    "    print('OK {}'.format(url))\n",
    "\n",
    "async def main(urls):\n",
    "    for url in urls:\n",
    "        await crawl_page(url)\n",
    "\n",
    "%time asyncio.run(main(['url_1', 'url_2', 'url_3', 'url_4']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d96fbac6-b8fe-4e24-a75d-fc2089fa4340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "crawling url_1\n",
      "crawling url_2\n",
      "crawling url_3\n",
      "crawling url_4\n",
      "OK url_1\n",
      "OK url_2\n",
      "OK url_3\n",
      "OK url_4\n",
      "CPU times: user 7.39 ms, sys: 5.14 ms, total: 12.5 ms\n",
      "Wall time: 4 s\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "\n",
    "async def crawl_page(url):\n",
    "    print('crawling {}'.format(url))\n",
    "    sleep_time = int(url.split('_')[-1])\n",
    "    await asyncio.sleep(sleep_time)\n",
    "    print('OK {}'.format(url))\n",
    "\n",
    "async def main(urls):\n",
    "    tasks = [asyncio.create_task(crawl_page(url)) for url in urls]\n",
    "    for task in tasks:\n",
    "        await task\n",
    "\n",
    "%time asyncio.run(main(['url_1', 'url_2', 'url_3', 'url_4']))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ef903f90-3087-4fb1-a1b7-1f379236d976",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "crawling url_1\n",
      "crawling url_2\n",
      "crawling url_3\n",
      "crawling url_4\n",
      "OK url_1\n",
      "OK url_2\n",
      "OK url_3\n",
      "OK url_4\n",
      "CPU times: user 7.99 ms, sys: 6.03 ms, total: 14 ms\n",
      "Wall time: 4 s\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "\n",
    "async def crawl_page(url):\n",
    "    print('crawling {}'.format(url))\n",
    "    sleep_time = int(url.split('_')[-1])\n",
    "    await asyncio.sleep(sleep_time)\n",
    "    print('OK {}'.format(url))\n",
    "\n",
    "async def main(urls):\n",
    "    tasks = [asyncio.create_task(crawl_page(url)) for url in urls]\n",
    "    await asyncio.gather(*tasks)\n",
    "\n",
    "%time asyncio.run(main(['url_1', 'url_2', 'url_3', 'url_4']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e0a2f800-113d-44e3-a831-e224647855e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, ZeroDivisionError('division by zero'), CancelledError('')]\n",
      "CPU times: user 1.75 ms, sys: 2.03 ms, total: 3.79 ms\n",
      "Wall time: 2 s\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "\n",
    "async def worker_1():\n",
    "    await asyncio.sleep(1)\n",
    "    return 1\n",
    "\n",
    "async def worker_2():\n",
    "    await asyncio.sleep(2)\n",
    "    return 2 / 0\n",
    "\n",
    "async def worker_3():\n",
    "    await asyncio.sleep(3)\n",
    "    return 3\n",
    "\n",
    "async def main():\n",
    "    task_1 = asyncio.create_task(worker_1())\n",
    "    task_2 = asyncio.create_task(worker_2())\n",
    "    task_3 = asyncio.create_task(worker_3())\n",
    "\n",
    "    await asyncio.sleep(2)\n",
    "    task_3.cancel()\n",
    "\n",
    "    res = await asyncio.gather(task_1, task_2, task_3, return_exceptions=True)\n",
    "    print(res)\n",
    "\n",
    "%time asyncio.run(main())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84aaf3de-972a-4760-905e-5215b214ebec",
   "metadata": {},
   "source": [
    "## 生产者消费者"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a3d1ad41-41b7-4954-a70c-d48a3ad436f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "producer producer_1 produce msg 7\n",
      "producer producer_2 produce msg 5\n",
      "comsumer consumer_1 consume msg 7\n",
      "comsumer consumer_2 consume msg 5\n",
      "producer producer_1 produce msg 10\n",
      "producer producer_2 produce msg 2\n",
      "comsumer consumer_1 consume msg 10\n",
      "comsumer consumer_2 consume msg 2\n",
      "producer producer_1 produce msg 1\n",
      "producer producer_2 produce msg 9\n",
      "comsumer consumer_1 consume msg 1\n",
      "comsumer consumer_2 consume msg 9\n",
      "producer producer_1 produce msg 5\n",
      "producer producer_2 produce msg 1\n",
      "comsumer consumer_1 consume msg 5\n",
      "comsumer consumer_2 consume msg 1\n",
      "producer producer_1 produce msg 5\n",
      "producer producer_2 produce msg 4\n",
      "comsumer consumer_1 consume msg 5\n",
      "comsumer consumer_2 consume msg 4\n",
      "producer producer_1 produce msg 1\n",
      "producer producer_2 produce msg 6\n",
      "comsumer consumer_1 consume msg 1\n",
      "comsumer consumer_2 consume msg 6\n",
      "producer producer_1 produce msg 1\n",
      "producer producer_2 produce msg 6\n",
      "comsumer consumer_1 consume msg 1\n",
      "comsumer consumer_2 consume msg 6\n",
      "producer producer_1 produce msg 8\n",
      "producer producer_2 produce msg 5\n",
      "comsumer consumer_1 consume msg 8\n",
      "comsumer consumer_2 consume msg 5\n",
      "producer producer_1 produce msg 10\n",
      "producer producer_2 produce msg 4\n",
      "comsumer consumer_1 consume msg 10\n",
      "comsumer consumer_2 consume msg 4\n",
      "producer producer_1 produce msg 8\n",
      "producer producer_2 produce msg 8\n",
      "comsumer consumer_1 consume msg 8\n",
      "comsumer consumer_2 consume msg 8\n",
      "CPU times: user 16.3 ms, sys: 8.01 ms, total: 24.3 ms\n",
      "Wall time: 10 s\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import asyncio\n",
    "\n",
    "async def consumer(queue, id):\n",
    "    while True:\n",
    "        val =  await queue.get()\n",
    "        print(\"comsumer {} consume msg {}\".format(id, val))\n",
    "        await asyncio.sleep(1)\n",
    "        \n",
    "\n",
    "async def producer(queue, id):\n",
    "    for i in range(10):\n",
    "        val = random.randint(1, 10)\n",
    "        await queue.put(val)\n",
    "        print(\"producer {} produce msg {}\".format(id, val))\n",
    "        await asyncio.sleep(1) \n",
    "\n",
    "async def main():\n",
    "    queue = asyncio.Queue()\n",
    "\n",
    "    consumer_1 = asyncio.create_task(consumer(queue, 'consumer_1'))\n",
    "    consumer_2 = asyncio.create_task(consumer(queue, 'consumer_2'))\n",
    "\n",
    "    producer_1 = asyncio.create_task(producer(queue, 'producer_1'))\n",
    "    producer_2 = asyncio.create_task(producer(queue, 'producer_2'))\n",
    "\n",
    "    await asyncio.sleep(10)\n",
    "    consumer_1.cancel()\n",
    "    consumer_2.cancel()\n",
    "    \n",
    "    await asyncio.gather(consumer_1, consumer_2, producer_1, producer_2, return_exceptions=True)\n",
    "\n",
    "%time asyncio.run(main())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e8b905-d3f2-4d9f-aee6-f7a2892f1dd6",
   "metadata": {},
   "source": [
    "## 实战：豆瓣近日推荐电影爬虫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5fd0bd16-6611-43e3-aa18-150c736257ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Collecting lxml\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/94/6a/42141e4d373903bfea6f8e94b2f554d05506dfda522ada5343c651410dc8/lxml-5.3.0-cp313-cp313-macosx_10_13_universal2.whl (8.2 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m47.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Installing collected packages: lxml\n",
      "Successfully installed lxml-5.3.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ebda2576-1832-4df7-a4a4-05da3eea8f80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "去唱卡拉OK吧！ 01月17日 1\n",
      "狗的审判 01月17日 1\n",
      "莫莉的冒险 01月18日 1\n",
      "真爱找麻烦！ 01月18日 1\n",
      "笑傲江湖 01月28日 1\n",
      "射雕英雄传：侠之大者 01月29日 1\n",
      "封神第二部：战火西岐 01月29日 1\n",
      "哪吒之魔童闹海 01月29日 1\n",
      "蛟龙行动 01月29日 1\n",
      "唐探1900 01月29日 1\n",
      "熊出没·重启未来 01月29日 1\n",
      "祭屋 01月30日 1\n",
      "美国队长4 02月14日 1\n",
      "我们的命中注定 02月14日 1\n",
      "真爱营业 02月14日 1\n",
      "多幸运遇见你 02月14日 1\n",
      "花样年华 02月 1\n",
      "7天 03月14日 1\n",
      "午夜怨灵 03月14日 1\n",
      "苍茫的天涯是我的爱 05月01日 1\n",
      "CPU times: user 35.4 ms, sys: 4.62 ms, total: 40 ms\n",
      "Wall time: 909 ms\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def main():\n",
    "    url = \"https://movie.douban.com/cinema/later/beijing/\"\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/521.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"}\n",
    "    init_page = requests.get(url, headers=headers).content\n",
    "    init_soup = BeautifulSoup(init_page, 'lxml')\n",
    "\n",
    "    all_movies = init_soup.find('div', id=\"showing-soon\")\n",
    "    if all_movies:\n",
    "        for each_movie in all_movies.find_all('div', class_=\"item\"):\n",
    "            all_a_tag = each_movie.find_all('a')\n",
    "            # print(all_a_tag)\n",
    "            \n",
    "            all_li_tag = each_movie.find_all('li')\n",
    "    \n",
    "            movie_name = all_a_tag[1].text\n",
    "            url_to_fetch = all_a_tag[1]['href']\n",
    "            movie_date = all_li_tag[0].text\n",
    "    \n",
    "            # response_item = requests.get(url_to_fetch).content\n",
    "            # soup_item = BeautifulSoup(response_item, 'lxml')\n",
    "            # img_tag = all_a_tag[0]['img']\n",
    "    \n",
    "            print('{} {} {}'.format(movie_name, movie_date, 1))\n",
    "    else:\n",
    "        print(\"all movies is none\")\n",
    "\n",
    "%time main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686cbea3-e730-448b-a8c1-e24fa0816ae3",
   "metadata": {},
   "source": [
    "# Futures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa09e780-5141-4007-a15c-f46b093d9e11",
   "metadata": {},
   "source": [
    "# Asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47b12f7-ef67-4078-939a-6f6a165bdffd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
